{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11513046,"sourceType":"datasetVersion","datasetId":7219633}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import the Libraries","metadata":{}},{"cell_type":"code","source":"! pip install PyPDF2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:22:18.665322Z","iopub.execute_input":"2025-04-24T10:22:18.665613Z","iopub.status.idle":"2025-04-24T10:22:21.911550Z","shell.execute_reply.started":"2025-04-24T10:22:18.665592Z","shell.execute_reply":"2025-04-24T10:22:21.910590Z"}},"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"! pip install langchain==0.1.14 langchain-core==0.1.38 pydantic==1.10.13\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:11:31.578032Z","iopub.execute_input":"2025-04-24T10:11:31.578264Z","iopub.status.idle":"2025-04-24T10:11:43.522007Z","shell.execute_reply.started":"2025-04-24T10:11:31.578240Z","shell.execute_reply":"2025-04-24T10:11:43.521313Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain==0.1.14\n  Downloading langchain-0.1.14-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-core==0.1.38\n  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\nCollecting pydantic==1.10.13\n  Downloading pydantic-1.10.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (2.0.38)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (3.11.16)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (1.33)\nCollecting langchain-community<0.1,>=0.0.30 (from langchain==0.1.14)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.14)\n  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.14)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.14) (2.32.3)\nCollecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.14)\n  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\nCollecting packaging<24.0,>=23.2 (from langchain-core==0.1.38)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==1.10.13) (4.13.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.19.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.14) (3.0.0)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community<0.1,>=0.0.30 (from langchain==0.1.14)\n  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.14) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.14) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.14) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.14) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.14) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.14) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (0.14.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (1.0.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1->langchain==0.1.14) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1->langchain==0.1.14) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1->langchain==0.1.14) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1->langchain==0.1.14) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1->langchain==0.1.14) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (1.3.1)\nDownloading langchain-0.1.14-py3-none-any.whl (812 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.38-py3-none-any.whl (279 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.2/279.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\nInstalling collected packages: tenacity, pydantic, packaging, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 9.0.0\n    Uninstalling tenacity-9.0.0:\n      Successfully uninstalled tenacity-9.0.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.3\n    Uninstalling pydantic-2.11.3:\n      Successfully uninstalled pydantic-2.11.3\n  Attempting uninstall: packaging\n    Found existing installation: packaging 24.2\n    Uninstalling packaging-24.2:\n      Successfully uninstalled packaging-24.2\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.3.8\n    Uninstalling langsmith-0.3.8:\n      Successfully uninstalled langsmith-0.3.8\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.35\n    Uninstalling langchain-core-0.3.35:\n      Successfully uninstalled langchain-core-0.3.35\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.6\n    Uninstalling langchain-text-splitters-0.3.6:\n      Successfully uninstalled langchain-text-splitters-0.3.6\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.18\n    Uninstalling langchain-0.3.18:\n      Successfully uninstalled langchain-0.3.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsigstore 3.6.1 requires pydantic<3,>=2, but you have pydantic 1.10.13 which is incompatible.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.13 which is incompatible.\nydata-profiling 4.16.1 requires pydantic>=2, but you have pydantic 1.10.13 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nalbumentations 2.0.4 requires pydantic>=2.9.2, but you have pydantic 1.10.13 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nwandb 0.19.6 requires pydantic<3,>=2.6, but you have pydantic 1.10.13 which is incompatible.\ngoogle-genai 0.8.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.13 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.38 langchain-text-splitters-0.0.2 langsmith-0.1.147 packaging-23.2 pydantic-1.10.13 tenacity-8.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Tuple\nimport torch\nimport PyPDF2\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport subprocess\nimport sys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:22:27.285546Z","iopub.execute_input":"2025-04-24T10:22:27.286445Z","iopub.status.idle":"2025-04-24T10:22:49.239136Z","shell.execute_reply.started":"2025-04-24T10:22:27.286414Z","shell.execute_reply":"2025-04-24T10:22:49.238563Z"}},"outputs":[{"name":"stderr","text":"2025-04-24 10:22:37.636752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745490157.830610      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745490157.892165      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:22:58.788220Z","iopub.execute_input":"2025-04-24T10:22:58.788801Z","iopub.status.idle":"2025-04-24T10:22:58.793231Z","shell.execute_reply.started":"2025-04-24T10:22:58.788777Z","shell.execute_reply":"2025-04-24T10:22:58.792553Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import subprocess\nimport sys\n\ndef install_if_needed(package_import_pairs):\n    for package, import_name in package_import_pairs:\n        try:\n            __import__(import_name)\n            print(f\"{package} already installed.\")\n        except ImportError:\n            print(f\"Installing {package}...\")\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# Define packages with their actual import names\npackages = [\n    (\"langchain\", \"langchain\"),              # just install latest langchain\n    (\"faiss-cpu\", \"faiss\"),                  # faiss-cpu for CPU environments like Kaggle\n    (\"ipywidgets\", \"ipywidgets\"),\n    (\"pydantic==1.10.13\", \"pydantic\")        # if needed for langchain compatibility\n]\n\ninstall_if_needed(packages)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:04.811405Z","iopub.execute_input":"2025-04-24T10:23:04.811678Z","iopub.status.idle":"2025-04-24T10:23:09.170595Z","shell.execute_reply.started":"2025-04-24T10:23:04.811654Z","shell.execute_reply":"2025-04-24T10:23:09.169942Z"}},"outputs":[{"name":"stdout","text":"langchain already installed.\nInstalling faiss-cpu...\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (23.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/30.7 MB 63.8 MB/s eta 0:00:00\nInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.10.0\nipywidgets already installed.\npydantic==1.10.13 already installed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/agriculturedatasetnewmeta/MetaHack\" # Default path for input files in Kaggle\nOUTPUT_DIR = \"/kaggle/working\" # Default path for output in Kaggle\nEMBEDDINGS_DIR = os.path.join(OUTPUT_DIR, \"embeddings\")\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:16.420868Z","iopub.execute_input":"2025-04-24T10:23:16.421826Z","iopub.status.idle":"2025-04-24T10:23:16.425739Z","shell.execute_reply.started":"2025-04-24T10:23:16.421796Z","shell.execute_reply":"2025-04-24T10:23:16.424899Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create output directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(EMBEDDINGS_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:18.327744Z","iopub.execute_input":"2025-04-24T10:23:18.328069Z","iopub.status.idle":"2025-04-24T10:23:18.333356Z","shell.execute_reply.started":"2025-04-24T10:23:18.328043Z","shell.execute_reply":"2025-04-24T10:23:18.332608Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def extract_text_from_pdfs(pdf_directory: str) -> Dict[str, str]:\n    \"\"\"\n    Extract text from all PDFs in the given directory\n    \"\"\"\n    pdf_texts = {}\n    \n    for filename in os.listdir(pdf_directory):\n        if filename.endswith('.pdf'):\n            file_path = os.path.join(pdf_directory, filename)\n            with open(file_path, 'rb') as file:\n                pdf_reader = PyPDF2.PdfReader(file)\n                text = \"\"\n                for page in pdf_reader.pages:\n                    text += page.extract_text() + \"\\n\"\n                pdf_texts[filename] = text\n    \n    return pdf_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:20.978509Z","iopub.execute_input":"2025-04-24T10:23:20.979050Z","iopub.status.idle":"2025-04-24T10:23:20.983681Z","shell.execute_reply.started":"2025-04-24T10:23:20.979024Z","shell.execute_reply":"2025-04-24T10:23:20.983128Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def preprocess_text(text: str) -> str:\n    \"\"\"\n    Clean and preprocess text from PDFs\n    \"\"\"\n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    # Remove header/footer patterns (customize based on your PDFs)\n    text = re.sub(r'Page \\d+ of \\d+', '', text)\n    return text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:23.658726Z","iopub.execute_input":"2025-04-24T10:23:23.659101Z","iopub.status.idle":"2025-04-24T10:23:23.663364Z","shell.execute_reply.started":"2025-04-24T10:23:23.659075Z","shell.execute_reply":"2025-04-24T10:23:23.662729Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_text_into_chunks(text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP) -> List[str]:\n    \"\"\"\n    Split text into manageable chunks for embedding\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len\n    )\n    return text_splitter.split_text(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:25.459722Z","iopub.execute_input":"2025-04-24T10:23:25.460118Z","iopub.status.idle":"2025-04-24T10:23:25.464485Z","shell.execute_reply.started":"2025-04-24T10:23:25.460095Z","shell.execute_reply":"2025-04-24T10:23:25.463846Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def create_embeddings(chunks: List[str], source_documents: List[str]) -> FAISS:\n    \"\"\"\n    Create embeddings for text chunks using HuggingFace embeddings and store in FAISS index\n    \"\"\"\n    # Use a HuggingFace embedding model that works well with FAISS\n    embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n    \n    # Configure to use GPU if available\n    model_kwargs = {'device': device}\n    \n    embeddings = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        model_kwargs=model_kwargs\n    )\n    \n    # Create metadata for each chunk\n    metadatas = [{\"source\": doc} for doc in source_documents]\n    \n    # Create and persist vector store\n    vectorstore = FAISS.from_texts(\n        texts=chunks,\n        embedding=embeddings,\n        metadatas=metadatas\n    )\n    \n    # Save the FAISS index\n    vectorstore.save_local(EMBEDDINGS_DIR)\n    \n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:23:27.570613Z","iopub.execute_input":"2025-04-24T10:23:27.571189Z","iopub.status.idle":"2025-04-24T10:23:27.576195Z","shell.execute_reply.started":"2025-04-24T10:23:27.571164Z","shell.execute_reply":"2025-04-24T10:23:27.575388Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def setup_retrieval_qa(vectorstore: FAISS) -> RetrievalQA:\n    # Create memory with an explicit output_key\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\", \n        return_messages=True,\n        output_key=\"result\"  # This tells the memory which key to store\n    )\n    \n    model_name = \"google/flan-t5-base\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_name, \n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, \n        device_map=\"auto\"\n    )\n    \n    pipe = pipeline(\n        \"text2text-generation\", \n        model=model, \n        tokenizer=tokenizer, \n        max_new_tokens=512, \n        temperature=0.3, \n        top_p=0.95\n    )\n    \n    llm = HuggingFacePipeline(pipeline=pipe)\n    \n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n        memory=memory,\n        return_source_documents=True\n    )\n    \n    return qa_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:13:58.816133Z","iopub.execute_input":"2025-04-24T11:13:58.816442Z","iopub.status.idle":"2025-04-24T11:13:58.822357Z","shell.execute_reply.started":"2025-04-24T11:13:58.816421Z","shell.execute_reply":"2025-04-24T11:13:58.821546Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def process_query(query: str, qa_chain: RetrievalQA) -> Tuple[str, List[str]]:\n    \"\"\"\n    Process user query and return response with source documents\n    \"\"\"\n    # Add specific prompt for agricultural financial assistance\n    enhanced_query = f\"\"\"\n    Query: {query}\n    \n    If this question is about agricultural financial assistance or government support schemes, \n    please provide detailed information about eligibility criteria, application process, \n    deadlines, and contact information if available. Focus on practical steps farmers can take \n    to access this support.\n    \"\"\"\n    \n    # Get response from QA chain\n    result = qa_chain(enhanced_query)\n    \n    answer = result[\"result\"]\n    source_docs = []\n    \n    # Extract source document references\n    if \"source_documents\" in result:\n        for doc in result[\"source_documents\"]:\n            if hasattr(doc, \"metadata\") and \"source\" in doc.metadata:\n                source = doc.metadata[\"source\"]\n                if source not in source_docs:\n                    source_docs.append(source)\n    \n    return answer, source_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:01.361774Z","iopub.execute_input":"2025-04-24T11:14:01.362492Z","iopub.status.idle":"2025-04-24T11:14:01.367217Z","shell.execute_reply.started":"2025-04-24T11:14:01.362468Z","shell.execute_reply":"2025-04-24T11:14:01.366409Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def build_knowledge_base(pdf_directory):\n    \"\"\"\n    Build the knowledge base from PDFs\n    \"\"\"\n    pdf_texts = extract_text_from_pdfs(pdf_directory)\n    all_chunks = []\n    source_documents = []\n    \n    for filename, text in pdf_texts.items():\n        processed_text = preprocess_text(text)\n        chunks = split_text_into_chunks(processed_text)\n        all_chunks.extend(chunks)\n        source_documents.extend([filename] * len(chunks))\n    \n    print(f\"Created {len(all_chunks)} text chunks from {len(pdf_texts)} documents\")\n    \n    # Create vector store\n    vectorstore = create_embeddings(all_chunks, source_documents)\n    \n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:03.691433Z","iopub.execute_input":"2025-04-24T11:14:03.692014Z","iopub.status.idle":"2025-04-24T11:14:03.696635Z","shell.execute_reply.started":"2025-04-24T11:14:03.691993Z","shell.execute_reply":"2025-04-24T11:14:03.695952Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def create_kaggle_ui():\n    \"\"\"\n    Create a simple UI using IPython widgets for Kaggle notebooks\n    \"\"\"\n    # Create output widget to display chat history\n    chat_output = widgets.Output()\n    \n    # Create input widget for user queries\n    query_input = widgets.Text(\n        value='',\n        placeholder='Ask about agricultural policies and financial support...',\n        description='Query:',\n        disabled=False,\n        layout=widgets.Layout(width='80%')\n    )\n    \n    # Create button to submit query\n    submit_button = widgets.Button(\n        description='Ask',\n        disabled=False,\n        button_style='primary',\n        tooltip='Submit your query',\n        icon='check'\n    )\n    \n    # Create button to reset chat\n    reset_button = widgets.Button(\n        description='Reset Chat',\n        disabled=False,\n        button_style='danger',\n        tooltip='Reset the chat history',\n        icon='refresh'\n    )\n    \n    # Create file upload widget for custom PDFs\n    upload_widget = widgets.FileUpload(\n        accept='.pdf',\n        multiple=True,\n        description='Upload PDFs',\n        layout=widgets.Layout(width='80%')\n    )\n    \n    # Display header\n    display(HTML(\"<h1>Agricultural Financial Assistance Chatbot</h1>\"))\n    display(HTML(\"<h3>Ask questions about agricultural policies and government support schemes</h3>\"))\n    \n    # Display widgets\n    input_row = widgets.HBox([query_input, submit_button, reset_button])\n    display(input_row)\n    display(upload_widget)\n    display(chat_output)\n    \n    # Store chat history\n    chat_history = []\n    \n    # Function to handle query submission\n    def handle_submit(button):\n        query = query_input.value\n        if not query:\n            return\n        \n        # Clear input field\n        query_input.value = ''\n        \n        with chat_output:\n            # Display user query\n            display(HTML(f\"<div style='background-color:#f0f0f0; padding:10px; margin:5px; border-radius:5px;'><b>You:</b> {query}</div>\"))\n            \n            # Process query and display response\n            display(HTML(\"<div style='padding:10px; margin:5px; border-radius:5px;'><b>Assistant:</b> <i>Searching through agricultural policies...</i></div>\"))\n            \n            try:\n                answer, sources = process_query(query, qa_chain)\n                \n                # Clear the \"Searching...\" message\n                clear_output(wait=True)\n                \n                # Display user query again (since we cleared output)\n                display(HTML(f\"<div style='background-color:#f0f0f0; padding:10px; margin:5px; border-radius:5px;'><b>You:</b> {query}</div>\"))\n                \n                # Display the answer\n                response_html = f\"<div style='padding:10px; margin:5px; border-radius:5px;'><b>Assistant:</b> {answer}\"\n                \n                if sources:\n                    response_html += \"<br><br><b>Sources:</b><ul>\"\n                    for source in sources:\n                        response_html += f\"<li>{source}</li>\"\n                    response_html += \"</ul>\"\n                \n                response_html += \"</div>\"\n                display(HTML(response_html))\n                \n                # Add to chat history\n                chat_history.append({\"role\": \"user\", \"content\": query})\n                chat_history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": sources})\n                \n            except Exception as e:\n                # Handle errors\n                clear_output(wait=True)\n                display(HTML(f\"<div style='background-color:#f0f0f0; padding:10px; margin:5px; border-radius:5px;'><b>You:</b> {query}</div>\"))\n                display(HTML(f\"<div style='color:red; padding:10px; margin:5px; border-radius:5px;'><b>Error:</b> {str(e)}</div>\"))\n    \n    # Function to handle chat reset\n    def handle_reset(button):\n        with chat_output:\n            clear_output()\n            chat_history.clear()\n            display(HTML(\"<div style='padding:10px; margin:5px; border-radius:5px;'><i>Chat history has been reset.</i></div>\"))\n    \n    # Function to handle file uploads\n    def handle_upload_change(change):\n        if not change.new:\n            return\n        \n        with chat_output:\n            for filename, file_info in change.new.items():\n                # Save the uploaded file\n                upload_path = os.path.join(OUTPUT_DIR, filename)\n                with open(upload_path, 'wb') as f:\n                    f.write(file_info['content'])\n                \n                display(HTML(f\"<div style='padding:10px; margin:5px; border-radius:5px;'><i>Uploaded {filename}. Rebuilding knowledge base...</i></div>\"))\n            \n            # Rebuild knowledge base with new PDFs\n            try:\n                global vectorstore, qa_chain\n                vectorstore = build_knowledge_base(OUTPUT_DIR)\n                qa_chain = setup_retrieval_qa(vectorstore)\n                display(HTML(\"<div style='padding:10px; margin:5px; border-radius:5px;'><i>Knowledge base updated successfully!</i></div>\"))\n            except Exception as e:\n                display(HTML(f\"<div style='color:red; padding:10px; margin:5px; border-radius:5px;'><b>Error rebuilding knowledge base:</b> {str(e)}</div>\"))\n    \n    # Register event handlers\n    submit_button.on_click(handle_submit)\n    reset_button.on_click(handle_reset)\n    upload_widget.observe(handle_upload_change, names='value')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:12.365846Z","iopub.execute_input":"2025-04-24T11:14:12.366514Z","iopub.status.idle":"2025-04-24T11:14:12.377947Z","shell.execute_reply.started":"2025-04-24T11:14:12.366492Z","shell.execute_reply":"2025-04-24T11:14:12.377408Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def main():\n    global DATA_DIR, vectorstore, qa_chain  # Declare global variables\n    \n    print(\"Initializing Agricultural Policy RAG Chatbot...\")\n    \n    # Check if the input directory exists, otherwise create it\n    if not os.path.exists(DATA_DIR):\n        print(f\"Input directory not found at {DATA_DIR}\")\n        print(f\"Creating directory at {OUTPUT_DIR}/temp_pdfs\")\n        os.makedirs(f\"{OUTPUT_DIR}/temp_pdfs\", exist_ok=True)\n        DATA_DIR = f\"{OUTPUT_DIR}/temp_pdfs\"\n        print(\"Please upload PDF files using the file upload widget\")\n    \n    # Create some dummy files if needed for testing\n    if len([f for f in os.listdir(DATA_DIR) if f.endswith('.pdf')]) == 0:\n        print(\"No PDF files found. The system will initialize with an empty knowledge base.\")\n        print(\"Please upload PDF files using the file upload widget to build the knowledge base.\")\n        sample_text = \"This is a sample agricultural policy document. It contains information about farm subsidies.\"\n        with open(os.path.join(OUTPUT_DIR, \"sample_policy.pdf\"), \"wb\") as f:\n            writer = PyPDF2.PdfWriter()\n            writer.add_blank_page(width=595, height=842)  # A4 size\n            writer.write(f)\n    \n    try:\n        # First, see if we can load from saved embeddings\n        if os.path.exists(EMBEDDINGS_DIR) and any(os.listdir(EMBEDDINGS_DIR)):\n            print(\"Loading existing knowledge base...\")\n            embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n            model_kwargs = {'device': device}\n            embeddings = HuggingFaceEmbeddings(\n                model_name=embedding_model_name,\n                model_kwargs=model_kwargs\n            )\n            vectorstore = FAISS.load_local(EMBEDDINGS_DIR, embeddings)\n            print(\"Knowledge base loaded successfully!\")\n        else:\n            print(\"Building knowledge base from PDFs...\")\n            vectorstore = build_knowledge_base(DATA_DIR)\n            print(\"Knowledge base built successfully!\")\n    except Exception as e:\n        print(f\"Error loading/building knowledge base: {str(e)}\")\n        embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n        model_kwargs = {'device': device}\n        embeddings = HuggingFaceEmbeddings(\n            model_name=embedding_model_name,\n            model_kwargs=model_kwargs\n        )\n        vectorstore = FAISS.from_texts(\n            texts=[\"This is a placeholder document for agricultural policies.\"],\n            embedding=embeddings,\n            metadatas=[{\"source\": \"placeholder.pdf\"}]\n        )\n    \n    qa_chain = setup_retrieval_qa(vectorstore)\n    \n    create_kaggle_ui()\n    \n    print(\"Agricultural Policy RAG Chatbot is ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:15.962822Z","iopub.execute_input":"2025-04-24T11:14:15.963429Z","iopub.status.idle":"2025-04-24T11:14:15.971368Z","shell.execute_reply.started":"2025-04-24T11:14:15.963404Z","shell.execute_reply":"2025-04-24T11:14:15.970604Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:18.997545Z","iopub.execute_input":"2025-04-24T11:14:18.998071Z","iopub.status.idle":"2025-04-24T11:14:19.001712Z","shell.execute_reply.started":"2025-04-24T11:14:18.998046Z","shell.execute_reply":"2025-04-24T11:14:19.000947Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"if __name__==\"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:14:21.864423Z","iopub.execute_input":"2025-04-24T11:14:21.865153Z","iopub.status.idle":"2025-04-24T11:14:25.476961Z","shell.execute_reply.started":"2025-04-24T11:14:21.865120Z","shell.execute_reply":"2025-04-24T11:14:25.476245Z"}},"outputs":[{"name":"stdout","text":"Initializing Agricultural Policy RAG Chatbot...\nLoading existing knowledge base...\nError loading/building knowledge base: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and no that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h1>Agricultural Financial Assistance Chatbot</h1>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Ask questions about agricultural policies and government support schemes</h3>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Ask about agricul…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b2be7c2f76481abacb5bab984a3cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.pdf', description='Upload PDFs', layout=Layout(width='80%'), multiple=True)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ce1923939544dab0c84371a3265be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0927f54eade44dc7bec5d4da22509178"}},"metadata":{}},{"name":"stdout","text":"Agricultural Policy RAG Chatbot is ready!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}